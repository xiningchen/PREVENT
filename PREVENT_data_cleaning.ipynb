{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Cleaning / Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.12 (default, Mar  2 2022, 12:59:08) \n",
      "[Clang 13.0.0 (clang-1300.0.27.3)]\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import importlib\n",
    "import sys\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import controllability as ctrb\n",
    "importlib.reload(ctrb)\n",
    "import Towlson_group_code.data_io as myFunc\n",
    "importlib.reload(myFunc)\n",
    "import Prevent.PREVENT_functions as prev_fct\n",
    "importlib.reload(prev_fct)\n",
    "\n",
    "print(sys.version)\n",
    "\n",
    "PICKLE_PATH = '../../PREVENT_Study/pickles/'\n",
    "CONNECTOME_DATA_PATH = '../../PREVENT_Study/data/Individual_VolumeNormalized_dataframes/'\n",
    "# CONNECTOME_DATA_PATH = '../PREVENT_Study/data/Individual_NONnormalized_dataframes/'\n",
    "FIGURE_PATH = '../../PREVENT_Study/figures/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Networks from Volume Normalized Connectomes\n",
    "- [x] Calculate controllability values for each network\n",
    "- [x] Record network metric values into graphs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loadBrainCSV(time, stabilize=True):\n",
    "    hc_data = {}\n",
    "    p_data = {}\n",
    "    pklName = f'{time}_connectomes.pkl'\n",
    "    bad_regions = []\n",
    "    if os.path.exists(PICKLE_PATH+pklName):\n",
    "        with open(PICKLE_PATH+pklName, 'rb') as f:\n",
    "            hc_data = pkl.load(f)\n",
    "            p_data = pkl.load(f)\n",
    "    else:\n",
    "        # regions to delete\n",
    "        delete_regions =  ['Optic-Chiasm', '3rd-Ventricle', 'CSF', '4th-Ventricle', 'Left-vessel', 'Right-vessel', 'Left-Lateral-Ventricle', 'Left-Inf-Lat-Vent', 'Right-Inf-Lat-Vent', 'Right-Lateral-Ventricle', 'Brain-Stem', 'CC_Posterior', 'CC_Mid_Posterior', 'CC_Central', 'CC_Mid_Anterior', 'CC_Anterior', 'Left-Cerebral-White-Matter', 'Left-Cerebellum-White-Matter', 'Left-Cerebellum-Cortex', 'Right-Cerebral-White-Matter', 'Right-Cerebellum-White-Matter', 'Right-Cerebellum-Cortex', 'Left-VentralDC', 'Left-choroid-plexus', 'Right-VentralDC', 'Right-choroid-plexus', 'WM-hypointensities', 'ctx-lh-unknown', 'ctx-rh-unknown']\n",
    "        delete_regions += ['Right-Pallidum', 'Left-Pallidum', 'ctx_lh_G_insular_short', 'ctx_rh_G_insular_short']\n",
    "\n",
    "        # Load all pickle files from data source folder\n",
    "        bad_files = 0\n",
    "        n_population = 0\n",
    "        hc_pop = 0\n",
    "        tia_pop = 0\n",
    "        stdOut = sys.stdout\n",
    "        sys.stdout = open(f'../../PREVENT_Study/data/{time}_error_log.txt', 'w')\n",
    "        for root, dirs, files in os.walk(CONNECTOME_DATA_PATH):\n",
    "            for file in (files):\n",
    "                if not file.endswith('.pickle'):\n",
    "                    continue\n",
    "                if time not in file:\n",
    "                    continue\n",
    "                # remove = ['174_bl_con', '087_Y5_con', '044_Y5_tia']\n",
    "                remove = []\n",
    "                skip = False\n",
    "                for r in remove:\n",
    "                    if r in file:\n",
    "                        skip = True\n",
    "                if skip:\n",
    "                    continue\n",
    "\n",
    "                with open(root+file, 'rb') as f:\n",
    "                    cdf = pkl.load(f)\n",
    "                # Clean up DF connectome\n",
    "                adjMatrix = cdf.drop(delete_regions, axis=0)\n",
    "                adjMatrix = adjMatrix.drop(delete_regions, axis=1)\n",
    "                if stabilize:\n",
    "                    adjMat = adjMatrix.to_numpy()\n",
    "                    if np.count_nonzero(adjMat) == 0:\n",
    "                        print(\"No brain data #\", patID)\n",
    "                        continue\n",
    "                    meanWeight = adjMat.sum() / np.count_nonzero(adjMat)\n",
    "                    adjMatrix = adjMatrix.div(meanWeight)\n",
    "                G = nx.from_pandas_adjacency(adjMatrix)\n",
    "                badGraph, G, badRegions = prev_fct.rank_nodes(G)\n",
    "                if badGraph:\n",
    "                    print(file[:10])\n",
    "                    bad_files += 1\n",
    "                    for b in badRegions:\n",
    "                        bad_regions.append(b)\n",
    "                        print('\\t*', b)\n",
    "                    continue\n",
    "                    # cdf.to_excel('../PREVENT_study/data/bad/'+file[:len(file)-7]+'.xlsx')\n",
    "\n",
    "                # Avg Controllability calc and ranking\n",
    "                avgCtrbDict = ctrb.avg_control(G)\n",
    "                nx.set_node_attributes(G, avgCtrbDict, name='avgCtrb')\n",
    "                badGraph, G, badRegions = prev_fct.rank_nodes(G, attr='avgCtrb')\n",
    "                if badGraph:\n",
    "                    print(f\"Could not rank avg ctrb for {file}\")\n",
    "\n",
    "                # Modal controllability calc and ranking\n",
    "                modalCtrbDict = ctrb.modal_control(G)\n",
    "                nx.set_node_attributes(G, modalCtrbDict, name='modCtrb')\n",
    "                badGraph, G, badRegions = prev_fct.rank_nodes(G, attr='modCtrb')\n",
    "                if badGraph:\n",
    "                    print(f\"Could not rank mod ctrb for {file}\")\n",
    "\n",
    "                info = file.split(\"_\")\n",
    "                patID = info[0]\n",
    "                if info[2] == 'tia':\n",
    "                    # store in p_data as a networkX graph\n",
    "                    p_data[patID] = G\n",
    "                    tia_pop += 1\n",
    "                else:\n",
    "                    hc_data[patID] = G\n",
    "                    hc_pop += 1\n",
    "                n_population += 1\n",
    "        with open(PICKLE_PATH+pklName, 'wb') as f:\n",
    "            pkl.dump(hc_data, f)\n",
    "            pkl.dump(p_data, f)\n",
    "        print('\\n-----------------------------------')\n",
    "        print(f'Summary for {time} data set: ')\n",
    "        print(f'There is a total of {n_population} good files.')\n",
    "        print(f'   * {hc_pop} are Control')\n",
    "        print(f'   * {tia_pop} are TIA')\n",
    "        print(f'There are {bad_files} bad files that contained isolated nodes (hence not saved to cleaned data pickle).')\n",
    "        print(f'In total the frequency of bad regions are: ')\n",
    "        freq = sorted(Counter(bad_regions).items(), key=lambda x: x[1], reverse=True)\n",
    "        print(*freq, sep=\"\\n\")\n",
    "\n",
    "        sys.stdout = stdOut\n",
    "    return hc_data, p_data, bad_regions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we have a pickle for each time frame, save them together in a single pickle."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hc_bl_data, p_bl_data, bl_bad_regions = loadBrainCSV(time='bl')\n",
    "hc_y1_data, p_y1_data, y1_bad_regions  = loadBrainCSV(time='Y1')\n",
    "hc_y3_data, p_y3_data, y3_bad_regions  = loadBrainCSV(time='Y3')\n",
    "hc_y5_data, p_y5_data, y5_bad_regions  = loadBrainCSV(time='Y5')\n",
    "\n",
    "individual_data = {'HCbl': hc_bl_data, 'Pbl': p_bl_data, 'HCy1': hc_y1_data, 'Py1': p_y1_data,\n",
    "                   'HCy3': hc_y3_data, 'Py3': p_y3_data, 'HCy5': hc_y5_data, 'Py5': p_y5_data}\n",
    "myFunc.save_to_pickle(individual_data, PICKLE_PATH, 'Normalized_Connectomes.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# For FSLeyes visualization data prep"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('../../PREVENT_Study/Brain_Atlas/abbrev_to_label_mapping.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "mapping = {}\n",
    "for line in lines:\n",
    "    s = line.split(' ')\n",
    "    if s[0] == '95':\n",
    "        break\n",
    "    lr = s[1].split('_')[-1]\n",
    "    abbrv = s[3].rstrip('\\n') + \".\" + lr\n",
    "    # print(abbrv, s[1])\n",
    "    mapping[abbrv] = s[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Volume Data\n",
    "- Store as pandas Dataframe pickles.\n",
    "- Rename columns to correspond to our network's node names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are more region names in the volume data sheets than the nodes we have in our brain networks.\n",
    "Load each volume data and only select the node regions that matches the nodes we have in our brain network."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    }
   ],
   "source": [
    "metadata, node_list = prev_fct.load_meta_data()\n",
    "column_list = []\n",
    "for n in node_list:\n",
    "    if n[:3] == 'ctx':\n",
    "        column_list.append(n[4:].replace(\"_and_\", \"&\")+\"_volume\")\n",
    "    elif 'Thalamus' in n:\n",
    "        column_list.append(n+'-Proper')\n",
    "    else:\n",
    "        column_list.append(n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "vol_df = myFunc.import_XLSX('../../PREVENT_Study/data/Region Volume data for participants/', 'Y5.xlsx')\n",
    "remove_columns = list(set(vol_df.columns) - set(column_list))\n",
    "vol_df = vol_df.drop(remove_columns, axis=1)\n",
    "rename_columns = {c: node_list[i]  for i ,c in enumerate(column_list)}\n",
    "vol_df.rename(columns=rename_columns, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n"
     ]
    }
   ],
   "source": [
    "print(159+179)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
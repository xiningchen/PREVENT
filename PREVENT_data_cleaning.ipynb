{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Cleaning / Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.12 (default, Mar  1 2023, 16:37:51) \n",
      "[Clang 13.1.6 (clang-1316.0.21.2.5)]\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import importlib\n",
    "import sys\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import controllability as ctrb\n",
    "importlib.reload(ctrb)\n",
    "import Towlson_group_code.data_io as myFunc\n",
    "importlib.reload(myFunc)\n",
    "import PREVENT_functions as prev_fct\n",
    "importlib.reload(prev_fct)\n",
    "\n",
    "print(sys.version)\n",
    "\n",
    "PICKLE_PATH = '../../PREVENT_Study/pickles/'\n",
    "# CONNECTOME_DATA_PATH = '../../PREVENT_Study/data/Individual_VolumeNormalized_dataframes/'\n",
    "CONNECTOME_DATA_PATH = '../../PREVENT_Study/data/Individual_NONnormalized_dataframes/'\n",
    "FIGURE_PATH = '../../PREVENT_Study/figures/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Networks from Volume Normalized Connectomes\n",
    "- [x] Calculate controllability values for each network\n",
    "- [x] Record network metric values into graphs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def loadBrainCSV(time, stabilize=True):\n",
    "    hc_data = {}\n",
    "    p_data = {}\n",
    "    # pklName = f'{time}_connectomes.pkl'\n",
    "    pklName = f'{time}_nn_connectomes.pkl'\n",
    "    bad_regions = []\n",
    "    if os.path.exists(PICKLE_PATH+pklName):\n",
    "        with open(PICKLE_PATH+pklName, 'rb') as f:\n",
    "            hc_data = pkl.load(f)\n",
    "            p_data = pkl.load(f)\n",
    "    else:\n",
    "        # regions to delete\n",
    "        delete_regions =  ['Optic-Chiasm', '3rd-Ventricle', 'CSF', '4th-Ventricle', 'Left-vessel', 'Right-vessel', 'Left-Lateral-Ventricle', 'Left-Inf-Lat-Vent', 'Right-Inf-Lat-Vent', 'Right-Lateral-Ventricle', 'Brain-Stem', 'CC_Posterior', 'CC_Mid_Posterior', 'CC_Central', 'CC_Mid_Anterior', 'CC_Anterior', 'Left-Cerebral-White-Matter', 'Left-Cerebellum-White-Matter', 'Left-Cerebellum-Cortex', 'Right-Cerebral-White-Matter', 'Right-Cerebellum-White-Matter', 'Right-Cerebellum-Cortex', 'Left-VentralDC', 'Left-choroid-plexus', 'Right-VentralDC', 'Right-choroid-plexus', 'WM-hypointensities', 'ctx-lh-unknown', 'ctx-rh-unknown']\n",
    "        delete_regions += ['Right-Pallidum', 'Left-Pallidum',\n",
    "                           'ctx_lh_G_insular_short', 'ctx_rh_G_insular_short',\n",
    "                           'ctx_lh_G_front_inf-Orbital', 'ctx_rh_G_front_inf-Orbital',\n",
    "                           'ctx_rh_G_Ins_lg_and_S_cent_ins', 'ctx_lh_G_Ins_lg_and_S_cent_ins',\n",
    "                           'ctx_rh_G_subcallosal', 'ctx_lh_G_subcallosal',\n",
    "                           'ctx_lh_S_interm_prim-Jensen', 'ctx_rh_S_interm_prim-Jensen']\n",
    "\n",
    "        # Load all pickle files from data source folder\n",
    "        bad_files = 0\n",
    "        n_population = 0\n",
    "        hc_pop = 0\n",
    "        tia_pop = 0\n",
    "        stdOut = sys.stdout\n",
    "        sys.stdout = open(f'../../PREVENT_Study/dump/{time}_error_log.txt', 'w')\n",
    "        for root, dirs, files in os.walk(CONNECTOME_DATA_PATH):\n",
    "            for file in (files):\n",
    "                if not file.endswith('.pickle'):\n",
    "                    continue\n",
    "                if time not in file:\n",
    "                    continue\n",
    "                remove = ['371_BL_Control', '063_BL_Control', '174_BL_Control', '036_BL_TIA', '058_Y1_Control', '241_Y1_TIA', '253_Y1_TIA', '087_Y5_Control', '044_Y5_TIA', '198_Y5_TIA']\n",
    "                skip = False\n",
    "                for r in remove:\n",
    "                    if r in file:\n",
    "                        skip = True\n",
    "                if skip:\n",
    "                    continue\n",
    "\n",
    "                with open(root+file, 'rb') as f:\n",
    "                    cdf = pkl.load(f)\n",
    "                # Clean up DF connectome\n",
    "                adjMatrix = cdf.drop(delete_regions, axis=0)\n",
    "                adjMatrix = adjMatrix.drop(delete_regions, axis=1)\n",
    "                G = nx.from_pandas_adjacency(adjMatrix)\n",
    "                if stabilize:\n",
    "                    adjMat = adjMatrix.to_numpy()\n",
    "                    if np.count_nonzero(adjMat) == 0:\n",
    "                        print(\"No brain data #\", patID)\n",
    "                        continue\n",
    "                    meanWeight = adjMat.sum() / np.count_nonzero(adjMat)\n",
    "                    adjMatrix = adjMatrix.div(meanWeight)\n",
    "                # --- Normalize G just to calculate controlllability\n",
    "                G_norm = nx.from_pandas_adjacency(adjMatrix)\n",
    "\n",
    "                # badGraph, G, badRegions = prev_fct.rank_nodes(G)\n",
    "                # if badGraph:\n",
    "                #     print(\"Can't rank node weights\", file[:10])\n",
    "                #     bad_files += 1\n",
    "                #     for b in badRegions:\n",
    "                #         bad_regions.append(b)\n",
    "                #         print('\\t*', b)\n",
    "                #     continue\n",
    "                    # cdf.to_excel('../PREVENT_study/data/bad/'+file[:len(file)-7]+'.xlsx')\n",
    "\n",
    "                # Avg Controllability calc and ranking\n",
    "                avgCtrbDict = ctrb.avg_control(G_norm)\n",
    "                nx.set_node_attributes(G, avgCtrbDict, name='avgCtrb')\n",
    "                # badGraph, G, badRegions = prev_fct.rank_nodes(G, attr='avgCtrb')\n",
    "                # if badGraph:\n",
    "                #     print(f\"Could not rank avg ctrb for {file}\")\n",
    "                #     bad_files += 1\n",
    "                #     for b in badRegions:\n",
    "                #         bad_regions.append(b)\n",
    "                #         print('\\t*', b)\n",
    "                #     continue\n",
    "\n",
    "                # Modal controllability calc and ranking\n",
    "                modalCtrbDict = ctrb.modal_control(G_norm)\n",
    "                nx.set_node_attributes(G, modalCtrbDict, name='modCtrb')\n",
    "                # badGraph, G, badRegions = prev_fct.rank_nodes(G, attr='modCtrb')\n",
    "                # if badGraph:\n",
    "                #     print(f\"Could not rank mod ctrb for {file}\")\n",
    "                #     bad_files += 1\n",
    "                #     for b in badRegions:\n",
    "                #         bad_regions.append(b)\n",
    "                #         print('\\t*', b)\n",
    "                #     continue\n",
    "\n",
    "                info = file.split(\"_\")\n",
    "                patID = info[0]\n",
    "                if info[2].lower() == 'tia':\n",
    "                    # store in p_data as a networkX graph\n",
    "                    p_data[patID] = G\n",
    "                    tia_pop += 1\n",
    "                else:\n",
    "                    hc_data[patID] = G\n",
    "                    hc_pop += 1\n",
    "                n_population += 1\n",
    "        with open(PICKLE_PATH+pklName, 'wb') as f:\n",
    "            pkl.dump(hc_data, f)\n",
    "            pkl.dump(p_data, f)\n",
    "        print('\\n-----------------------------------')\n",
    "        print(f'Summary for {time} data set: ')\n",
    "        print(f'There is a total of {n_population} good files.')\n",
    "        print(f'   * {hc_pop} are Control')\n",
    "        print(f'   * {tia_pop} are TIA')\n",
    "        print(f'There are {bad_files} bad files that contained isolated nodes (hence not saved to cleaned data pickle).')\n",
    "        print(f'In total the frequency of bad regions are: ')\n",
    "        freq = sorted(Counter(bad_regions).items(), key=lambda x: x[1], reverse=True)\n",
    "        print(*freq, sep=\"\\n\")\n",
    "\n",
    "        sys.stdout = stdOut\n",
    "    return hc_data, p_data, bad_regions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we have a pickle for each time frame, save them together in a single pickle."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hc_bl_data, p_bl_data, bl_bad_regions = loadBrainCSV(time='bl')\n",
    "hc_y1_data, p_y1_data, y1_bad_regions  = loadBrainCSV(time='Y1')\n",
    "hc_y3_data, p_y3_data, y3_bad_regions  = loadBrainCSV(time='Y3')\n",
    "hc_y5_data, p_y5_data, y5_bad_regions  = loadBrainCSV(time='Y5')\n",
    "\n",
    "individual_data = {'HCbl': hc_bl_data, 'Pbl': p_bl_data, 'HCy1': hc_y1_data, 'Py1': p_y1_data,\n",
    "                   'HCy3': hc_y3_data, 'Py3': p_y3_data, 'HCy5': hc_y5_data, 'Py5': p_y5_data}\n",
    "myFunc.save_to_pickle(individual_data, PICKLE_PATH, 'Normalized_Connectomes.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# For FSLeyes visualization data prep"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('../../PREVENT_Study/Brain_Atlas/abbrev_to_label_mapping.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "mapping = {}\n",
    "for line in lines:\n",
    "    s = line.split(' ')\n",
    "    if s[0] == '95':\n",
    "        break\n",
    "    lr = s[1].split('_')[-1]\n",
    "    abbrv = s[3].rstrip('\\n') + \".\" + lr\n",
    "    # print(abbrv, s[1])\n",
    "    mapping[abbrv] = s[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Volume Data\n",
    "- Store as pandas Dataframe pickles.\n",
    "- Rename columns to correspond to our network's node names\n",
    "- Everyone has different brain size. When you analyze region volume data you need to control for each person's varying brain volume.\n",
    "Total Intracranial Volume (TIV) should be the same for each person regardless of aging effects. Therefore when we deal with\n",
    "volume data, we should normalize each individual's nodal volume with TIV. There should be 1 TIV value per person and it should stay the same over the years.\n",
    "In the {time}_volume.pkl files, these volumes have been normalized with the patient's TIV value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are more region names in the volume data sheets than the nodes we have in our brain networks.\n",
    "Load each volume data and only select the node regions that matches the nodes we have in our brain network."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    }
   ],
   "source": [
    "metadata, node_list = prev_fct.load_meta_data()\n",
    "column_list = []\n",
    "for n in node_list:\n",
    "    if n[:3] == 'ctx':\n",
    "        column_list.append(n[4:].replace(\"_and_\", \"&\")+\"_volume\")\n",
    "    elif 'Thalamus' in n:\n",
    "        column_list.append(n+'-Proper')\n",
    "    else:\n",
    "        column_list.append(n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [X] Given the proper select of node volume data, normalize by the individual's TIV value.\n",
    "- [X] Store dataframe as a pkl to use later."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [55, 91, 175, 178, 181]\n",
      "Saved to ../../PREVENT_Study/pickles/y5_volume_tiv_normalized.pkl.\n"
     ]
    }
   ],
   "source": [
    "# vol_df = myFunc.import_XLSX('../../PREVENT_Study/data/Region Volume data for participants/', 'Y5.xlsx')\n",
    "# remove_columns = list(set(vol_df.columns) - set(column_list))\n",
    "# vol_df = vol_df.drop(remove_columns, axis=1)\n",
    "# rename_columns = {c: node_list[i]  for i ,c in enumerate(column_list)}\n",
    "# vol_df.rename(columns=rename_columns, inplace=True)\n",
    "t = 'y5'\n",
    "bl_tiv = myFunc.import_XLSX('../../PREVENT_Study/data/', 'BL_TIV.xlsx')\n",
    "vol_df = myFunc.load_from_pickle(PICKLE_PATH, f'{t}_volume.pkl')\n",
    "key_error_list = []\n",
    "for ri, row in vol_df.iterrows():\n",
    "    try:\n",
    "        p_tiv = bl_tiv.loc[ri][0]\n",
    "        vol_df.loc[ri] = row/p_tiv\n",
    "    except KeyError:\n",
    "        key_error_list.append(ri)\n",
    "print(len(key_error_list), key_error_list)\n",
    "myFunc.save_to_pickle(vol_df, PICKLE_PATH, f'{t}_volume_tiv_normalized.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Non Volume Normalized Connectomes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Process non volume normalized connectomes just like the normalized ones.\n",
    "Once we have a pickle for each time frame, save them together in a single pickle.\n",
    "- [X] Removed ranking of nodes based on edge values.\n",
    "- [X] Removed the 4 nodes that were giving issues in ranking before. Removed these 4 nodes even though we removed ranking\n",
    "is because otherwise I need to create a new node_list and redo the volume data as well. Eventually this should be done,\n",
    "but very quickly right now, just ignoring this in case we do decide to go back to ranking/volume normalized connectomes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../PREVENT_Study/pickles/Non_Normalized_Connectomes_2.pkl.\n"
     ]
    }
   ],
   "source": [
    "hc_bl_data, p_bl_data, bl_bad_regions = loadBrainCSV(time='BL')\n",
    "hc_y1_data, p_y1_data, y1_bad_regions  = loadBrainCSV(time='Y1')\n",
    "hc_y3_data, p_y3_data, y3_bad_regions  = loadBrainCSV(time='Y3')\n",
    "hc_y5_data, p_y5_data, y5_bad_regions  = loadBrainCSV(time='Y5')\n",
    "\n",
    "individual_data = {'HCbl': hc_bl_data, 'Pbl': p_bl_data, 'HCy1': hc_y1_data, 'Py1': p_y1_data,\n",
    "                   'HCy3': hc_y3_data, 'Py3': p_y3_data, 'HCy5': hc_y5_data, 'Py5': p_y5_data}\n",
    "myFunc.save_to_pickle(individual_data, PICKLE_PATH, 'Non_Normalized_Connectomes_2.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cross check result to look for isolated nodes. Manually check to see if these nodes are truly isolated (no edges)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bl HC 178\n",
      "\t success:  178\n",
      "y1 HC 77\n",
      "\t success:  77\n",
      "y3 HC 36\n",
      "\t success:  36\n",
      "y5 HC 33\n",
      "\t success:  33\n",
      "bl P 231\n",
      "\t success:  231\n",
      "y1 P 100\n",
      "\t success:  100\n",
      "y3 P 17\n",
      "\t success:  17\n",
      "y5 P 34\n",
      "\t success:  34\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "bad_people = []\n",
    "remove_list = {'HCbl': [], 'Pbl': [], 'HCy1': [], 'Py1': [],\n",
    "               'HCy3': [], 'Py3': [], 'HCy5': [], 'Py5': []}\n",
    "for patient_type in ['HC', 'P']:\n",
    "    # fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    if patient_type == 'HC':\n",
    "        color = \"tab:blue\"\n",
    "    if patient_type == 'P':\n",
    "        color = 'tab:orange'\n",
    "    for time in ['bl', 'y1', 'y3', 'y5']:\n",
    "        data = individual_data[patient_type+time]\n",
    "        print(time, patient_type, len(data))\n",
    "        num_success = 0\n",
    "        for pid, G in data.items():\n",
    "            success, G, badRegions = prev_fct.rank_nodes(G, 'weight')\n",
    "            if success == False:\n",
    "                remove_list[patient_type+time].append(pid)\n",
    "                # if len(badRegions) > 2:\n",
    "                #     remove_list[patient_type+time].append(pid)\n",
    "                # else:\n",
    "                #     bad_people.append((patient_type+time, pid))\n",
    "                    # print(pid, badRegions)\n",
    "            else:\n",
    "                num_success += 1\n",
    "        print(\"\\t success: \", num_success)\n",
    "\n",
    "bad_regions = []\n",
    "for a, b in bad_people:\n",
    "    bad_person = individual_data[a][b]\n",
    "    edges = bad_person.degree(weight='weight')\n",
    "    for k, v in edges:\n",
    "        if v == 0:\n",
    "            bad_regions.append(k)\n",
    "            avgC = nx.get_node_attributes(bad_person, \"avgCtrb\")[k]\n",
    "            modC = nx.get_node_attributes(bad_person, \"modCtrb\")[k]\n",
    "            # print(a, b, k, v, avgC, modC)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HCbl 0 []\n",
      "Pbl 0 []\n",
      "HCy1 0 []\n",
      "Py1 0 []\n",
      "HCy3 0 []\n",
      "Py3 0 []\n",
      "HCy5 0 []\n",
      "Py5 0 []\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(*Counter(bad_regions).most_common(), sep=\"\\n\")\n",
    "amap = {'HC': 'Control', 'P': 'TIA'}\n",
    "skip_list = []\n",
    "for k, v in remove_list.items():\n",
    "    print(k, len(v), v)\n",
    "    if len(k) == 4:\n",
    "        ptype = k[:2]\n",
    "        time = k[2:]\n",
    "    else:\n",
    "        ptype = k[:1]\n",
    "        time = k[1:]\n",
    "    for l in v:\n",
    "        skip_list.append(f'{l}_{time.upper()}_{amap[ptype]}')\n",
    "\n",
    "print(skip_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "G = individual_data['HCbl']['277']\n",
    "node_df = pd.DataFrame(data={'node names': G.nodes})\n",
    "node_df.to_excel('NodeList.xlsx', sheet_name='Sheet1', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Total cohort numbers by category."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P 218\n",
      "HC 185\n"
     ]
    }
   ],
   "source": [
    "metadata, node_list = prev_fct.load_meta_data()\n",
    "\n",
    "print('P', len(metadata[(metadata['C/T'] == 'P')]))\n",
    "print('HC', len(metadata[(metadata['C/T'] == 'HC')]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merging spreadsheets...\n",
    "Combine Age_Dates_and_Time sheet with ages_and_diagnoses. Ages_Dates_and_Time sheet contains the date and number of\n",
    "days between the time point scans. This information is needed to properly annualize or to be factored into our models\n",
    "when examining correlations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "og_sheet = myFunc.import_XLSX('../../PREVENT_Study/data/', 'ages_and_diagnoses.xlsx', index_col=None)\n",
    "new_sheet = myFunc.import_XLSX('../../PREVENT_Study/data/', 'Age_Dates_and_Time.xlsx', index_col=None)\n",
    "new_sheet.drop(columns=['Gender', 'Age', 'C/T'], inplace=True)\n",
    "merged_sheet = og_sheet.merge(new_sheet, how=\"left\" ,left_on='Subject ID', right_on='Subject ID', suffixes=('_og', '_new'))\n",
    "merged_sheet.sort_values(by=\"Subject ID\", inplace=True)\n",
    "merged_sheet = merged_sheet.reset_index().drop(columns='index')\n",
    "merged_sheet.to_excel('../../PREVENT_Study/data/ages_and_diagnoses_2.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Date: March 23, 2023\n",
    "\n",
    "Currently we should have scan dates + days between scans for Y5 and BL cohorts. Check that this is true or identify\n",
    "missing data points."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "metadata, node_list = prev_fct.load_meta_data()\n",
    "individual_data = myFunc.load_from_pickle(PICKLE_PATH, 'Non_Normalized_Connectomes.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "y5_ids = set()\n",
    "for k in individual_data.keys():\n",
    "    if len(k) == 4:\n",
    "        ptype = k[:2]\n",
    "        t = k[2:]\n",
    "    else:\n",
    "        ptype = k[0]\n",
    "        t = k[1:]\n",
    "    if t in ['bl', 'y1', 'y3']:\n",
    "        continue\n",
    "    for pid, G in individual_data[k].items():\n",
    "        y5_ids.add(pid)\n",
    "\n",
    "metadata.drop(columns=[\"Unnamed: 0\", \"MRI Date Y1\", \"MRI Date Y3\", \"Time Between BL and 1-Yr\", \"Time Between 1-Yr and 3-yr\", \"Time Between 3-Yr and 5-Yr\", \"Time Between BL and 3-Yr\"], inplace=True)\n",
    "x = metadata.loc[list(y5_ids)]\n",
    "x.to_excel('../../PREVENT_Study/dump/missing_dates.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}